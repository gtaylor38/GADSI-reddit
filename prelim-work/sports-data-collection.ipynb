{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52bff520-db2f-49e9-9126-4ebcbca16ec2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba90064-5bd2-404e-827f-7b7a9c36b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import requests.auth\n",
    "import json\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eafefb-f527-411b-b679-85d72454d819",
   "metadata": {},
   "source": [
    "### Get Authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4edc713e-3d65-4b2d-8e8b-718af0b310b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': '2205940059860-Pu6brezMzlKjawNDYJFoqCJnXx8eFg',\n",
       " 'token_type': 'bearer',\n",
       " 'expires_in': 86400,\n",
       " 'scope': '*'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# request a token\n",
    "client_auth = requests.auth.HTTPBasicAuth('ZfnXA82bMZkXWBRp_29tqw', 'MTr-tnMjbizJ9jeuPQI8EK-CiPUFWA')\n",
    "post_data = {'grant_type': 'password', 'username': 'gtaylor38gadsi', 'password': 'axfig654$Bin'}\n",
    "headers = {'User-Agent': 'PostScraperScript/1.0 by gtaylor38gadsi'}\n",
    "response_token = requests.post('https://www.reddit.com/api/v1/access_token', auth=client_auth, data=post_data, headers=headers)\n",
    "response_token.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e734d45c-4c21-4329-b27d-985324228ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store credentials\n",
    "access_token = response_token.json()['access_token']\n",
    "token_type = response_token.json()['token_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e4f44fa-3b38-4750-82a3-e7808c1c3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update headers for collection\n",
    "headers = {\"Authorization\": f'{token_type} {access_token}', 'User-Agent': 'PostScraperScript/1.0 by gtaylor38gadsi'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d140b9b-7386-40b1-8677-e4eb68874476",
   "metadata": {},
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78a83f40-0bc0-46ae-b7eb-3c0456fea10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize df\n",
    "posts_df = pd.DataFrame(columns=['title', 'subreddit', 'score', 'author_flair_text', 'link_flair_text', 'created_utc', 'num_comments'])\n",
    "\n",
    "# set base url\n",
    "url_base = 'https://oauth.reddit.com/new/.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8124f426-84b3-48e5-91b0-5dbff10a7e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make initial request, set time, initialize page list, define columns to scrape\n",
    "response = requests.get(url_base, headers=headers)\n",
    "curr_time = time.time()\n",
    "page_new = []\n",
    "base_cols = ['title', 'subreddit', 'score', 'author_flair_text', 'link_flair_text', 'created_utc', 'num_comments']\n",
    "\n",
    "# get data from first page using base url and store in posts_df\n",
    "for i in range(response.json()['data']['dist']):\n",
    "    row = [response.json()['data']['children'][i]['data'][col_name] for col_name in base_cols]\n",
    "    page_new.append(row)\n",
    "df = pd.DataFrame(page_new, columns=posts_df.columns)\n",
    "df['age'] = curr_time - df['created_utc']\n",
    "posts_df = pd.concat([posts_df, df])\n",
    "\n",
    "# collect data from next 39 pages\n",
    "for i in range(39):\n",
    "    # update after and url_current\n",
    "    after = response.json()['data']['after']\n",
    "    url_current = f'{url_base}?after={after}'\n",
    "\n",
    "    # make new request, update time, and reset page list\n",
    "    response = requests.get(url_current, headers=headers)\n",
    "    curr_time = time.time()\n",
    "    page_new = []\n",
    "\n",
    "    # collect and store data in page list\n",
    "    for j in range(response.json()['data']['dist']):\n",
    "        row = [response.json()['data']['children'][j]['data'][col_name] for col_name in posts_df.columns if col_name != 'age']\n",
    "        page_new.append(row)\n",
    "\n",
    "    # store page list as df, add age column, and concat df to posts_df\n",
    "    df = pd.DataFrame(page_new, columns=base_cols)\n",
    "    df['age'] = curr_time - df['created_utc']\n",
    "    posts_df = pd.concat([posts_df, df])\n",
    "    \n",
    "# report number of unique posts\n",
    "len(posts_df.title.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783ef7e9-5365-4d75-854a-07bd63fde402",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "671d1ba9-9934-43b3-9a85-d9d291fcafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write posts_df to csv:\n",
    "posts_df.to_csv('data/sports_posts3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
